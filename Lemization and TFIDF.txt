Lemmatization with NLTK :
	-> i hate the NLTK API but it's would be fun
	-> The lematization is the way to have standards words like better : good. The lemme of better is good.
	-> it's usefull because with one string we could get a list and convert this list in set to remove all occurences of a word.
	-> Lemmatization is a way to normalize the word

Source :
https://www.geeksforgeeks.org/python-lemmatization-with-nltk/#:~:text=Lemmatization%20is%20the%20process%20of,similar%20meaning%20to%20one%20word


Vectorisation of data : https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76
Why we use TFIDF : https://monkeylearn.com/blog/what-is-tf-idf/#:~:text=TF%2DIDF%20enables%20us%20to,in%20a%20machine%20learning%20algorithm.

TFIDF : 
	-> for work with data the computer need to have some integer so we need to convert into number the strings.
	-> This method call Bag of word
	-> So for a string we convert her in a list and we transform this list in a dictionnary with this template : {'word': occurences of the word}
	-> but after this part we need to clean the data and remove all stopwords
	-> TF : (Term Frequency) number of time appear in the text/ number of word in the text.
		* Exemple : 'I want to ride my bicycle, I want to ride my bike, I want to ride my bicycle'. TF(Bicycle) = 2/18, because we have 2 times bicyle and 18 words in this corpus.
	-> IDF : The log of the number of documents divided by the number of documents that contain the word w. We compute only one time for all documents the IDF
	-> The TFIDF is just the product of Tf by IDF
	-> Sklearn is a better way to compute the TFIDF but it's interessant to know how implement the TFIDF
	-> Sklearn is a library get many function and methods for make AI.